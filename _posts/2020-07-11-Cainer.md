---
layout:     post   				    # 使用的布局（不需要改）
title:      Introduction to docker 				# 标题 
subtitle:   Concepts and usage
date:       2020-07-11 				# 时间
author:     Li Ju 						# 作者
header-img: img/post-bg-2015.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
mathjax: true
tags:								#标签
    Cloud_Computing
---
This article will introduce some basic concepts and usage of Docker. 

## Virtualization & Containerization
Before talking about container or Docker, the concept of virtualization is of vital importance. Virtualization in 
computer science basically means creating a virtual version of something: you can virtualize hardware, operating system
or application at different levels, and everything based on this virtual version of "something" will not
know if they are on real or virtual "something". It is similar to abstraction in OOP: things in higher level will
not know and care about how functions are implements at lower levels. 

Generally there are 3 levels of virtualization: server virtualizaion(hardware virtualization), operating system 
virtualization and destop virtualization. 
1. Server virtualization is the virtualization of hardware: a series of virtual hardwares are offered and a real 
operating system over this can be installed. Virtual machine is an example at this l2. Operating system virtualization is aimed to offering applications a virtual OS: container techniques is an example
at this level; 
3. In the concept of destop virtualization, client desktop and computers which are used to run applications can be 
separated: one can use a virtual destop running on his/her own computer and command applications running on a remote
server computers. Remote desktop is an example at this level. 

#### Virtual machine
Hypervisor is a software which can manage and offer virtual resources to higher level. 
Generally there are two types of virtual machine based on where the hypervisor is based. 
![avatar](/img/20-07-11/VMs.png)

How privileged instructions from applications get executed by hardware? Originally for software-assisted full 
virtualization, application/system will not know they are on virtual hardware, and all their 
instructions will be passed to guest OS first, then guest OS passes these instructions to the supervisor, 
and finally supervisor will "translate" their instructions to real hardware. There is a huge performance loss
due to the extra "translation". Therefore an approach called para-virtualization is developed. Some modifications are
done for guest OS so that privileged instructions can be directly passed to real hardware. However modifications for
each OS on different hardware differ, which makes systems unportab. In recent years, a new technology offered by 
CPU manufactures can achieve hardware-assisted full virtualization: CPU itself can figure privileged instructions 
from guest OS and handle them without help of hypervisors. Hardware-assisted full virtualization has been the new 
trend for virtualization techniques. 

#### Container
Comparing with virtual machine, container is a virtualization technique on higher level: they are based on system 
kernels as shown following. 
![avatar](/img/20-07-11/vm_ctn.png)
Virtual machines isolate operating systems while containers only isolate user part of operating system and share 
system kernels. They seem similar but actually they aim differently: what virtual machine is required is the whole
isolated operatisystem, based on which development, application deployment and others could be done. However
containers is only aimed to RUN an application properly with it dependencies. Therefore container is much light-weight: 
only target application and its dependencies are required while virtual machine requires an entire complete operating
system. 

## Docker: architecture and objects
The concept of container has been there for decades like CoreOS, LXC linux containers, OpenVZ and of course, Docker. 
Docker is a linux container technique taking advantages of linux features: 
1. Namespace: namespace of linux is used to isolate environment of different containers: every container is facing
bare new system; 
2. Control group: cgroup is used to manage resources of hardware: resources could be assigned to different containers
and monitor the usage; 
3. Union File System: UFS is used to reduce redundancy of numerous containers: only files which are modified will be
created/replicated while those are only read will be shared. 

#### Architecture
Docker is consisted of three parts:  server and registry as shown as following: 
![avatar](/img/20-07-11/arch.png)
Client is used by user to command server and client can be installed remotely or locally on host machine. Server is 
the application on host machine to maintain containers. And remote registry is a database to save container templates. 

#### Objects
To know better how Docker works, objects of Docker should be known. Workflow of Docker is shown as following: 
![avatar](/img/20-07-11/object.png)
The core object of Docker is Docker image. which can be seen as a template of containers. We can run containers
from an image and after some modification for the container, a new image could be saved from a container. Normally
there are two ways to get Docker image: we can pull an image from registry and push your own image to registry, or we
can create a Docker image from a Dockerfile, which is like an instruction telling how to make an image. Also as-created
containers can be stopped and restarted, also containers can export a snapshot, and reversely snapshot can be imported
to get a container. 

All file change in containers will not affect host system, in another word, all writing will be removed after containers
are removed. Therefore, how to do valid modifications on host machine? Also, it is always required to share data 
between containers, host machine and external storage. There are three commonly-used approaches: volume, bind mounting 
and other plugins. 
1. Volume is folders managed by Docker Engine, and they will not be deleted when containers are destroyed. So container
can stored data in volume and volume data can also be shared between containers; 
2. Like linux mounting, bind mounting can mount directory of host system to a virtual directory in containers so that 
containers can write on that area in host system. Actually volume is just a specified bindingg mounting limited 
to folders in Docker directory. 
3. To access external storage like USB or external disk, some other plugins can be used. 

Also network is also required for ontainers: one can map real host system's port to container's virtual port so that
container can be communicated via host system's port. 

## Dockerfile: how to write elegant Dockerfile
Though images could be pulled from registry, however, to customize our own image in which our own application is able
to run, Dockerfile is the most important. Before we write Dockerfile, we need to know how Docker build images from 
Dockerfile. 

#### Mechanism
1. Every image is based on one base image and following modifications are made on the base image. In Dockerfile, 
keyword ``FROM`` is used to specify base image; 
2. Image is built layer by layer: Each single instruction in Dockerfile will add a new layer to the image stack. 
Commonly-used instructions include : ``LABEL`` to add some information about the image, ``RUN`` to run a command
in image, ``EXPOSE`` to expose a port of container to network, ``ENV`` tp specify an environment variable for 
container, ``ADD/COPY`` to add or copy file to container, ``VOLUME`` to attach a volume to container, ``WORKDIR``
to specify a working directory for container, ``ENTRYPOINT`` and ``CMD`` to determine the first command when a 
container entity is created from an image. A container will be stopped when the task specified in ``ENTRYPOINT`` 
and ``CMD`` is finished. 

For ``ENTRYPOINT`` and ``CMD``, there are two different ways to write and they will decide commands will be run 
in shell mode or execution mode. 

``ENTRYPOINT ["command", "argument_1", "argument_2", ...]`` enables execution mode, 
which means the first process running in a container is the command you specified in ``ENTRYPOINT``. `CMD command argument_1 argument_2...`` enables shell mode, which means the first process running in a newly-create
container is /bin/sh, and then the command is run in the shell. Following is a image illustrating what exact command
is run in two different mode with different form. 
![avatar](/img/20-07-11/mode.png)

#### Rules for writing Dockerfile
Based on the mechanism of image building, here are several rules one should try to follow when creaing Dockerfile: 
1. No unneccessary package: image should be minimal-sized and abundant packages should not be contained; 
2. Decouple applications: it is always good to isolate different services into different containers rather than 
multiple services running in a single container: multiple simple containers are easier to handle and reuse; 
3. Minimize number of layers: as image/containers are built layer by layer, minimizing number of layer is good for
reuse and reduce size of images; 
4. For multi-line arguments, it is always a good habit to sort them in a alphabetical order: this will make your life
easier when modifying your old image; 
5. Use leverage cache: image use Union File System, we can reuse layer in cache which are build by other containers
in the host system; 
5. Use multi-stage build: multi-stage build is a new feature of Dockerfile. Traditionally, if one need to build an
environment for an application running, two separated containers should be built: one for compiling and another for
running, also a shell script is required to save dependencies compiled by 1st container into the second container. 
It is very low efficient. Here multi-stage Dockerfile could be used shown as following: 
![avatar](/img/20-07-11/multistage.png)

## Docker orchestration
Docker is always used in very large scale scenarios. How can we deploy containers for different applications on 
numerous machines in a faster way and make them work together in a certain way? This is basically called docker
orchestration. Docker officially released 3 software (isolated from Docker Engine itself): 
1. Docker Machine: it can provision hosts and install Docker Engine in a simple way; 
2. Docker Swarm: it can cluster multiple Docker hosts under a single host and deploy containers on the single node, 
which will also make containers deployed on host clusters; (K8s is also aimed to do so)
3. Docker Compose: it can deploy multi-container applications in a fast way. 

